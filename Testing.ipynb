{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bF_9ANXfT7s2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from pathlib import Path\n",
    "import scipy\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import copy,os,sys\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ndiYHfsUIf5"
   },
   "outputs": [],
   "source": [
    "# Normalizes x using mean and std\n",
    "# dimension of x= (w,h,c)\n",
    "def normalize(x,mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]):\n",
    "    mean=torch.tensor(mean)\n",
    "    std=torch.tensor(std)\n",
    "    return (x-mean)/std\n",
    "\n",
    "def denormalize(x,mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]):\n",
    "    mean=torch.tensor(mean)\n",
    "    std=torch.tensor(std)\n",
    "    return x*std+mean\n",
    "\n",
    "# changes dim of x from (w,h,3)->(1,3,w,h) to easily pass through model\n",
    "def img2passable(x):\n",
    "    x=x.permute((2,0,1))\n",
    "    x=x.unsqueeze(0)\n",
    "    return x\n",
    "\n",
    "# inverse of img_to_passable\n",
    "# converts (1,3,w,h) to (w,h,3)\n",
    "def passable2img(x):\n",
    "    x=x.squeeze(0)\n",
    "    x=x.permute((1,2,0))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ur_0ceHXULld"
   },
   "outputs": [],
   "source": [
    "class Resnet_backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Resnet_backbone,self).__init__()\n",
    "        resnet50=torchvision.models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Freeze all the layers\n",
    "        for param in resnet50.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Get feature vector from last layer \n",
    "        self.get_feature=nn.Sequential(*list(resnet50.children())[:-1])\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.get_feature(x)\n",
    "        return x\n",
    "\n",
    "class Siamese(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Siamese,self).__init__()\n",
    "        self.resnet=Resnet_backbone()\n",
    "        self.exif_features=nn.Sequential(\n",
    "                            nn.Linear(4096,2048),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(2048,1024),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(1024,163))  \n",
    "\n",
    "    def forward(self,x,y):\n",
    "        x=self.resnet(x).view(-1,2048)\n",
    "        y=self.resnet(y).view(-1,2048)\n",
    "        out=torch.cat((x,y),axis=1)   # To combine feature vectors of both image\n",
    "        out=self.exif_features(out)\n",
    "        return out\n",
    "\n",
    "# To get consistency of the two patches\n",
    "class Consistency(nn.Module):\n",
    "    # siamese is pretrained siamese network to get exif features\n",
    "    def __init__(self,siamese):\n",
    "        super(Consistency,self).__init__()\n",
    "        self.siamese=siamese\n",
    "\n",
    "        # # Freeze all the layers\n",
    "        # for param in self.siamese.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        self.get_consistency=nn.Sequential(\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(163,512),  \n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(512,2))\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        x=self.siamese(x,y)\n",
    "        out=self.get_consistency(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Load the EXIF model and then consistency model\n",
    "\n",
    "exif_model=Siamese().to(device)\n",
    "saved_model_path = 'exif_model.pth'\n",
    "\n",
    "if device==torch.device('cuda'):\n",
    "    checkpoint = torch.load(saved_model_path)\n",
    "else:\n",
    "    checkpoint = torch.load(saved_model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "exif_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model = Consistency(exif_model).to(device)\n",
    "\n",
    "saved_model_path = 'consistency_model.pth'\n",
    "if device==torch.device('cuda'):\n",
    "    checkpoint = torch.load(saved_model_path)\n",
    "else:\n",
    "    checkpoint = torch.load(saved_model_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "apQ3wA__WEzk"
   },
   "outputs": [],
   "source": [
    "test_transform=transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])               # normalize by ImageNet statistics\n",
    "\n",
    "# Load the image given image path\n",
    "# output- tensor of size (1,c,w,h)\n",
    "def load_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    if np.max(img)>1:\n",
    "        img = img/255\n",
    "    img = test_transform(img)\n",
    "    return img.float().unsqueeze(0)\n",
    "    \n",
    "\n",
    "# Gets the height and width of the image after stride \n",
    "def get_stride_new_dim(img, stride):\n",
    "    stride_h, stride_w = stride\n",
    "    h, w = img.size()[2:]\n",
    "    h_new = (h-128)//stride_h + 1 \n",
    "    w_new = (w-128)//stride_w + 1 \n",
    "    return h_new, w_new\n",
    "\n",
    "\n",
    "# Get new dimension of stride to reduce the computation \n",
    "# max_size is the max size of output after applying stride\n",
    "def get_new_stride(img, stride, max_size=25):\n",
    "    if isinstance(stride, tuple):\n",
    "        stride_h, stride_w = stride\n",
    "    else:\n",
    "        stride_h, stride_w = stride, stride\n",
    "\n",
    "    h, w = img.size()[2:]\n",
    "    flag = True           # h > w\n",
    "    if max(h,w)==w:\n",
    "        h,w = w,h\n",
    "        flag = False\n",
    "    \n",
    "    stride_h = max(stride_h, np.int((h-128)/(max_size-1)))\n",
    "    w_max_size = round(max_size*w/h)\n",
    "    stride_w = max(stride_w, np.int((w-128)/(w_max_size-1)))\n",
    "\n",
    "    if flag==False:\n",
    "        stride_h, stride_w = stride_w, stride_h\n",
    "    return (stride_h, stride_w)\n",
    "\n",
    "\n",
    "# Get all patches from the image of size (128,128) in a convolutional manner\n",
    "def get_patches(img, stride, batch_size):\n",
    "    a=F.unfold(img, kernel_size=(128,128), stride=stride)\n",
    "    a=a.view(img.size(0),3,128,128,-1)\n",
    "    a=a.transpose(-1,0).squeeze(-1).to(device)\n",
    "    a=torch.split(a, batch_size)\n",
    "    return a\n",
    "\n",
    "\n",
    "# Reshape the consistency map to get a 2D matrix\n",
    "def reshape_map(img, c_map, stride):\n",
    "    h_new, w_new = get_stride_new_dim(img, stride)\n",
    "    c_map =  c_map.view(-1, h_new, w_new)\n",
    "    return c_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nuAe-ymCWO8-"
   },
   "outputs": [],
   "source": [
    "# Get all the consistency maps by comparing each patch in first_patch\n",
    "# to each patch in sec_patch (these maps show relative values to first_patch)\n",
    "def get_response_maps(img, first_patch, sec_patch, stride):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        response_maps=[]\n",
    "        for patch in first_patch:\n",
    "            prob_list=[]\n",
    "            for patchb in sec_patch:\n",
    "                patcha = patch.repeat(patchb.size(0),1,1,1)\n",
    "                out = model(patcha, patchb)\n",
    "                out = F.softmax(out, dim=1)\n",
    "                out = out[:,1]\n",
    "                prob_list.append(out)\n",
    "            prob_list = torch.cat(prob_list)\n",
    "            response_maps.append(prob_list)\n",
    "        response_maps = torch.stack(response_maps)\n",
    "        response_maps = reshape_map(img, response_maps, stride)\n",
    "    return response_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21Bu8gLpYLNR"
   },
   "outputs": [],
   "source": [
    "def Mean_Shift(points_, iters=5):\n",
    "    points = points_.reshape(-1, points_.shape[1]*points_.shape[2])\n",
    "    kdt = scipy.spatial.cKDTree(points)\n",
    "    eps_5 = np.percentile(scipy.spatial.distance.cdist(points, points, metric='euclidean'), 10)\n",
    "\n",
    "    for epis in range(iters):\n",
    "        for point_ind in range(points.shape[0]):\n",
    "            point = points[point_ind]\n",
    "            nearest_inds = kdt.query_ball_point(point, r=eps_5)\n",
    "            points[point_ind] = np.mean(points[nearest_inds], axis=0)\n",
    "    val = []\n",
    "\n",
    "    for i in range(points.shape[0]):\n",
    "        val.append(kdt.count_neighbors(scipy.spatial.cKDTree(np.array([points[i]])), r=eps_5))\n",
    "    mode_ind = np.argmax(val)\n",
    "    ind = np.nonzero(val == np.max(val))\n",
    "    return np.mean(points[ind[0]], axis=0).reshape(points_.shape[-2], points_.shape[-1])\n",
    "\n",
    "\n",
    "def Normalized_Cut(res):\n",
    "    sc = SpectralClustering(n_clusters=2, n_jobs=-1, affinity=\"precomputed\")\n",
    "    out = sc.fit_predict(res.reshape(-1, res.shape[1]*res.shape[2]))\n",
    "    vis = out.reshape((res.shape[1], res.shape[2]))\n",
    "    return vis\n",
    "\n",
    "\n",
    "# Cluster the maps by applying mean shift and normalized cuts\n",
    "def get_meanshift_ncut(response_maps, img):\n",
    "    res = np.copy(response_maps)\n",
    "    ms = Mean_Shift(res)\n",
    "    ncuts = Normalized_Cut(res)\n",
    "\n",
    "    # If most of the image is high probability, then flip it\n",
    "    if (np.mean(ms > .5) > .5):\n",
    "        ms = 1 - ms\n",
    "\n",
    "    if np.mean(ncuts > .5) > .5:\n",
    "        ncuts = 1 - ncuts\n",
    "\n",
    "    out_ms = cv2.resize(ms, (img.size(-1), img.size(-2)), interpolation=cv2.INTER_LINEAR)\n",
    "    out_ncuts = cv2.resize(ncuts.astype(np.float32), (img.size(-1), img.size(-2)), interpolation=cv2.INTER_LINEAR)\n",
    "    return out_ms, out_ncuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QqQU0Xk8YPJf"
   },
   "outputs": [],
   "source": [
    "# Gives the response maps\n",
    "def test(img=None, img_path=None, stride=6, batch_size=8, max_size=25):\n",
    "    if img_path is not None:\n",
    "        img = load_image(img_path)\n",
    "    if len(img.size())==3:\n",
    "        img = img2passable(img)\n",
    "    \n",
    "    stride = get_new_stride(img, stride, max_size=max_size)\n",
    "    first_patch = get_patches(img, stride, batch_size)\n",
    "    first_patch = torch.cat(first_patch)\n",
    "    sec_patch = get_patches(img, stride, batch_size)\n",
    "    \n",
    "    response_maps = get_response_maps(img, first_patch, sec_patch, stride)\n",
    "    response_maps = response_maps.cpu().detach().numpy()\n",
    "\n",
    "    mean_shift, ncuts = get_meanshift_ncut(response_maps, img)\n",
    "    return mean_shift, ncuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWl7G9V5ZF5b"
   },
   "outputs": [],
   "source": [
    "img_path = 'test.jpg'\n",
    "\n",
    "stride = (6,6)       # determines the resolution of the result\n",
    "batch_size = 1024    # Change depending on GPU capacity\n",
    "max_size = 25        # Determines the maximum resolution of consistency map\n",
    "\n",
    "img = load_image(img_path)\n",
    "print(\"Shape of the image:\", tuple(passable2img(img).shape))\n",
    "mean_shift, ncut = test(img, stride = stride, batch_size = batch_size, max_size = max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXkbba_hZJcB"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(20,10))\n",
    "# fig.tight_layout()\n",
    "fig.subplots_adjust(wspace=0.01)\n",
    "img = denormalize(passable2img(img)).numpy()\n",
    "ncut = ncut.repeat(3,axis=-1).reshape(-1, ncut.shape[1],3)\n",
    "\n",
    "ax[0].imshow(img)\n",
    "ax[0].set_title('Original Image')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(mean_shift, cmap='jet', vmin=0, vmax=1)\n",
    "ax[1].set_title('Mean Shift')\n",
    "ax[1].axis('off')\n",
    "\n",
    "alpha=0.2   # Change this to 0.8 if smaller spliced region\n",
    "ax[2].imshow((alpha*ncut+(1-alpha)*(1-ncut))*img, vmin=0, vmax=1)\n",
    "ax[2].set_title('Normalized Cut')\n",
    "ax[2].axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SML Project Testing",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
